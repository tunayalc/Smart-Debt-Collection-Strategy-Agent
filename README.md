# Intelligent Debt Collection Strategy Agent (with RL)

This project develops a debt collection strategy for financial institutions using Reinforcement Learning (RL) to maximize profitability. The RL agent learns to dynamically select the most suitable communication channel (SMS, Call, Wait) for each customer based on their status (debt amount, days overdue, etc.).

The project provides a modern AI solution to the challenge of balancing cost minimization with collection maximization.

## Problem Statement

Traditional debt collection systems often rely on static rules. This project treats each customer as a unique case and dynamically answers questions such as:
- Which customers should we call, and which only need a reminder?
- When should we be proactive, and when is it more profitable to wait?
- When is it worth taking the risk of customer churn?

The agent discovers the most profitable strategy by learning through trial and error across thousands of simulations.

## Technologies and Architecture

- **Programming Language:** Python  
- **RL Libraries:**
  - `Gymnasium`: For the custom RL environment simulating customer behavior and business rules.
  - `Stable-Baselines3`: For implementing the industry-standard Deep Q-Network (DQN) algorithm.
- **Data Management:** `Pandas` & `Numpy`

At the heart of the architecture is a simulation environment modeling customer personas and their responses to different communication channels.

### RL Design

- **State:** `[Debt Amount, Days Overdue, Payment History Score]`
- **Actions:** `[Send SMS, Send Email, Make Call, Wait]`
- **Reward Strategy:** A carefully designed reward-punishment mechanism to shape the agent’s behavior toward "smart profitability":
  - **High Reward:** Successful collection (extra bonus if achieved via call).
  - **High Penalty:** Customer churn or debt expiring.
  - **Low Cost:** Operational cost for each communication attempt.

## Results: An Intelligent and Profitable Agent

After rigorous reward engineering and model training, the agent successfully learned a highly effective and profitable strategy. Results from testing across 500 different customer scenarios are as follows:

| Metric                   | Result          | Comment                                                                 |
| ------------------------- | --------------- | ----------------------------------------------------------------------- |
| **Average Total Reward**  | **+162.84**     | The agent’s strategy is highly profitable.                               |
| **Successful Collection Rate** | **86.2%**   | Debts were successfully collected from the vast majority of customers.   |
| **Expiration Rate**       | **4.4%**        | The problem of passively missing opportunities was nearly eliminated.    |
| **Customer Churn Rate**   | **9.4%**        | The agent takes calculated risks for overall profitability.              |

### Strategy Learned by the Agent

- **Make Call (69.1%):** Learned that calling is the most powerful and effective tool and adopted it as the main strategy.  
- **Wait (27.0%):** Knows when to hold back instead of immediately acting, showing “smart” strategy.  
- **Send SMS (3.9%):** Used only in very specific, low-risk scenarios.  

## Project Structure and Files

The project consists of modular Python scripts, each fulfilling a specific role.

- **`DataFrame.py`**: Generates the core dataset `synthetic_customer_data.csv`. Creates logical and consistent data based on different customer personas to simulate realistic scenarios.

- **`debt_collection_env.py`**: Builds the custom simulation environment using `Gymnasium`. Defines the “game rules,” reward-punishment mechanism, and customer behavior shaping the agent’s learning.

- **`train.py`**: Trains the RL agent from scratch using `Stable-Baselines3`. Produces the trained model file `dqn_debt_collector.zip`.

- **`evaluate.py`**: Tests the trained agent (`dqn_debt_collector.zip`). Evaluates its performance across hundreds of customer scenarios and reports the results.

- **`dqn_debt_collector.zip`**: The trained, ready-to-use model generated by `train.py`.

- **`synthetic_customer_data.csv`**: The synthetic dataset created by `DataFrame.py`, used by the agent during training and testing.

## Portfolio Value

This project goes beyond traditional classification tasks, showcasing the ability to model a dynamic, strategic business problem using Reinforcement Learning.
